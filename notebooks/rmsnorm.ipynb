{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Benchmarking Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: since triton compiler is involved, we must restart the kernel after each modification of the optimized.rmsnorm module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from baselines.rmsnorm import RMSNormL3, RMSNormGT1, MyRMSNorm\n",
    "# Note, since triton compiler is involved, we must restart the kernel after each modification of the \n",
    "# optimized.rmsnorm module\n",
    "from optimized.rmsnorm import RMSNormTriton\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printRMSNorm(rmsnorm_class, x : torch.Tensor = None):\n",
    "    print(\"--------------------\")\n",
    "    print(\"RMSNormTriton: x.shape: \", end=\"\")\n",
    "    print(x.shape)\n",
    "    \n",
    "    # Compute the expected output\n",
    "    rmsnorm_test = rmsnorm_class(dim=3)\n",
    "    expected_output = rmsnorm_test.forward(x)\n",
    "    print(x)\n",
    "    print(expected_output)\n",
    "\n",
    "printRMSNorm(RMSNormGT1, torch.tensor([[1, 2, 3], [3, 3, 3], [7, 8, 9], [100, 150, 150]], dtype=torch.float32))\n",
    "printRMSNorm(RMSNormGT1, torch.tensor([[1, 2, 3], ], dtype=torch.float16))\n",
    "printRMSNorm(RMSNormGT1, torch.tensor([1, 2, 3], dtype=torch.float16))\n",
    "printRMSNorm(RMSNormGT1, torch.tensor([[[1, 2, 3], [7, 8, 9]], [[100, 150, 150], [100, 150, 150]]], dtype=torch.float16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printRMSNorm(rmsnorm_class, x : torch.Tensor = None):\n",
    "    print(\"--------------------\")\n",
    "    print(\"RMSNormTriton: x.shape: \", end=\"\")\n",
    "    print(x.shape)\n",
    "    # Compute the expected output\n",
    "    rmsnorm_test = rmsnorm_class(dim=3).cuda()\n",
    "    expected_output = rmsnorm_test.forward(x.cuda())\n",
    "    print(x)\n",
    "    print(expected_output)\n",
    "\n",
    "# One sequence\n",
    "printRMSNorm(RMSNormTriton, torch.tensor([[1, 2, 3], ], dtype=torch.float16))\n",
    "\n",
    "# Multiple sequences\n",
    "printRMSNorm(RMSNormTriton, torch.tensor([[1, 2, 3], [3, 3, 3], [7, 8, 9], [100, 150, 150]], dtype=torch.float32))\n",
    "\n",
    "# Multiple batches, with multiple sequences\n",
    "printRMSNorm(RMSNormTriton, torch.tensor([[[1, 2, 3], [7, 8, 9]], [[100, 150, 150], [100, 150, 150]]], dtype=torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSNormTriton: x.shape: \", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Performance Measurment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that outputs match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "\n",
    "# Make sure the numbers are the same (or close)\n",
    "torch.manual_seed(0)\n",
    "seq_len = 100\n",
    "model_dim = 4096\n",
    "x = torch.randn(seq_len, model_dim, device='cuda')\n",
    "\n",
    "# Triton implementation\n",
    "rmsnormtriton = RMSNormTriton(dim=model_dim).cuda()\n",
    "triton_output = rmsnormtriton.forward(x)\n",
    "\n",
    "# Ground Truth PyTorch implementation\n",
    "rmsnormgt1 = RMSNormGT1(dim=model_dim).cuda()\n",
    "torch_output = rmsnormgt1.forward(x)\n",
    "if torch.allclose(triton_output, torch_output):\n",
    "# assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of sequence leng to Sweep\n",
    "x_vals = [1, 128, 256] + [512 * i for i in range(1, 25)]\n",
    "\n",
    "# Value of model_dim\n",
    "model_dim = model_dim # from cell above since we pre-allocate the \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_name = 'runtime'\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['SeqLen'],  # argument names to use as an x-axis for the plot\n",
    "        # I want to benchmark sequence lengths from 1 to 12,800 \n",
    "        # llama2 context size is 8k tokens, but longer context len extensions are possible\n",
    "        # also I observed that it is the only way to saturate GPU memory BW of V100\n",
    "        # I want to include 1,128,and 256 to see the effect of small sequence lengths\n",
    "        x_vals = x_vals,\n",
    "\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=[\n",
    "            'triton',\n",
    "            'torch-native',\n",
    "            'torch-compile'\n",
    "        ],  # possible values for `line_arg``\n",
    "        line_names=[\n",
    "            \"Triton\",\n",
    "            \"Torch (native)\",\n",
    "            \"Torch (compiled)\"\n",
    "        ],  # label name for the lines\n",
    "\n",
    "        styles=[('blue', '-'), ('green', '-.'), ('green', '--')],  # line styles\n",
    "        # ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        ylabel=\"ms\",  # label name for the y-axis\n",
    "        plot_name=plot_name,  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'N': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "\n",
    "def benchmark(SeqLen, N, provider):\n",
    "    x = torch.randn(SeqLen, N, device='cuda', dtype=torch.float16)\n",
    "    quantiles = [0.5, 0.2, 0.8] # report median, 20th and 80th percentiles\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: rmsnormtriton(x), quantiles=quantiles)\n",
    "    if provider == 'torch-native':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: rmsnormgt1(x), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.compile(rmsnormgt1)(x), quantiles=quantiles)\n",
    "\n",
    "    # For GB/s reports\n",
    "    # gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    # return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "    # For ms reports\n",
    "    return ms, max_ms, min_ms\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=False, print_data=True, save_path='./prof/')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_name = 'bandwidth'\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['SeqLen'],  # argument names to use as an x-axis for the plot\n",
    "        # I want to benchmark sequence lengths from 1 to 12,800 \n",
    "        # llama2 context size is 8k tokens, but longer context len extensions are possible\n",
    "        # also I observed that it is the only way to saturate GPU memory BW of V100\n",
    "        # I want to include 1,128,and 256 to see the effect of small sequence lengths\n",
    "        x_vals = x_vals,\n",
    "\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=[\n",
    "            'triton',\n",
    "            'torch-native',\n",
    "            'torch-compile'\n",
    "        ],  # possible values for `line_arg``\n",
    "        line_names=[\n",
    "            \"Triton\",\n",
    "            \"Torch (native)\",\n",
    "            \"Torch (compiled)\"\n",
    "        ],  # label name for the lines\n",
    "\n",
    "        styles=[('blue', '-'), ('green', '-.'), ('green', '--')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"bandwidth\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        # ylabel=\"ms\",  # label name for the y-axis\n",
    "        # plot_name=\"runtime\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'N': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "\n",
    "def benchmark(SeqLen, N, provider):\n",
    "    x = torch.randn(SeqLen, N, device='cuda', dtype=torch.float16)\n",
    "    quantiles = [0.5, 0.2, 0.8] # report median, 20th and 80th percentiles\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: rmsnormtriton(x), quantiles=quantiles)\n",
    "    if provider == 'torch-native':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: rmsnormgt1(x), quantiles=quantiles)\n",
    "    if provider == 'torch-compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.compile(rmsnormgt1)(x), quantiles=quantiles)\n",
    "\n",
    "    # For GB/s reports\n",
    "    gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "    # For ms reports\n",
    "    # return ms, max_ms, min_ms\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=False, print_data=True, save_path='./prof/')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_len = 4096\n",
    "model_dim = 4096\n",
    "\n",
    "def runRMSNorm(rmsnorm_class, x : torch.Tensor = None):\n",
    "    rmsnorm_test = rmsnorm_class(dim=model_dim).cuda()\n",
    "    rmsnorm_test.forward(x.cuda())\n",
    "\n",
    "# In this example with wait=1, warmup=1, active=2, repeat=1,\n",
    "# profiler will skip the first step/iteration,\n",
    "# start warming up on the second, record\n",
    "# the third and the forth iterations,\n",
    "# after which the trace will become available\n",
    "# and on_trace_ready (when set) is called;\n",
    "# the cycle repeats starting with the next step\n",
    "wait_val = 1\n",
    "warmup_val = 1\n",
    "active_val = 2\n",
    "repeat_val = 1\n",
    "\n",
    "def trace_handler(prof):\n",
    "    \"\"\"trace_handler is called every time a new trace becomes available\"\"\"\n",
    "\n",
    "    print(prof.key_averages().table(\n",
    "        sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
    "    prof.export_chrome_trace(\"prof/rmsnorm_trace_{}x{}_{}.json\".format(\n",
    "        seq_len, model_dim, prof.step_num))\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        # torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "\n",
    "    schedule=torch.profiler.schedule(wait=wait_val, \n",
    "                                     warmup=warmup_val, \n",
    "                                     active=active_val, \n",
    "                                     repeat=repeat_val),\n",
    "    on_trace_ready=trace_handler\n",
    "    # Outputting for tensorboard? Use this instead:\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./prof')\n",
    "    ) as p:\n",
    "        for iter in range(10):\n",
    "            x = torch.randn(seq_len, model_dim, device='cuda', dtype=torch.float16)\n",
    "            runRMSNorm(RMSNormTriton, x)\n",
    "            p.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_cuda_time_in_ms(the_table_str: str):\n",
    "    for line in the_table_str.splitlines():\n",
    "        if \"Self CUDA time total:\" in line:\n",
    "            match = re.search(r\"(\\d+\\.\\d+)(\\w+)\", line)\n",
    "            if match:\n",
    "                time_value = float(match.group(1))\n",
    "                unit = match.group(2)\n",
    "                if unit == 'us':  # if unit is microseconds, convert to milliseconds\n",
    "                    time_value /= 1000\n",
    "                return time_value\n",
    "\n",
    "# Use the function\n",
    "cuda_time_in_ms = get_cuda_time_in_ms(p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=1))\n",
    "print(cuda_time_in_ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Artifacts with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "experiment_name = \"rmsnorm\"\n",
    "implementation_name = \"RMSNormTriton\"\n",
    "implementation_type = \"triton\"\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if(True):\n",
    "    # List of files to log\n",
    "    files_to_log = ['../optimized/rmsnorm.py', \n",
    "                    'prof/bandwidth.csv', \n",
    "                    'prof/bandwidth.png', \n",
    "                    'prof/runtime.csv', \n",
    "                    'prof/runtime.png',\n",
    "                    'prof/rmsnorm_trace_4096x4096_4.json'\n",
    "                    ]\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run(experiment_id=experiment.experiment_id) as run:\n",
    "        # Iterate over the list of files\n",
    "        for file in files_to_log:\n",
    "            # Log each file as an artifact\n",
    "            mlflow.log_artifact(file)\n",
    "        \n",
    "        # Log experiment with seq_len and model_dim, and result cuda_time_in_ms\n",
    "        mlflow.log_param(\"seq_len\", seq_len)\n",
    "        mlflow.log_param(\"model_dim\", model_dim)\n",
    "        mlflow.log_param(\"implementation\", implementation_name)\n",
    "        mlflow.log_param(\"implementation_type\", implementation_type)\n",
    "        mlflow.log_metric(\"cuda_time_in_ms\", cuda_time_in_ms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
